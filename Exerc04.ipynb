{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UpEaVQcgi5s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entre as versões do gradiente descendente (GD) que discutimos (batch, estocástico e mini-batch), qual deles chega mais rapidamente à vizinhança da solução ótima? Qual deles realmente converge? O que você pode fazer para que os outros também convirjam?"
      ],
      "metadata": {
        "id": "9XUCKe6fgjkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resposta: Entre as variantes do gradiente descendente, o gradiente estocástico (SGD) é geralmente o mais rápido para alcançar a vizinhança da solução ótima, pois atualiza os pesos com base em apenas um exemplo por vez, introduzindo aleatoriedade que acelera o processo inicial de aproximação. No entanto, essa mesma aleatoriedade o torna mais instável, dificultando a convergência exata ao ponto de mínimo global.\n",
        "\n",
        "A versão que apresenta maior estabilidade e convergência garantida é o gradiente descendente em batelada (batch), que utiliza todo o conjunto de dados para calcular o gradiente, resultando em passos mais consistentes em direção ao mínimo. No entanto, esse método tende a ser mais lento, especialmente em grandes volumes de dados.\n",
        "\n",
        "Para melhorar a convergência do SGD, podem ser aplicadas técnicas como momentum, que suaviza as atualizações, e decaimento da taxa de aprendizagem, que reduz gradualmente o passo de atualização. Já no caso do mini-batch gradient descent, um equilíbrio entre desempenho e estabilidade pode ser alcançado ajustando o tamanho do mini-batch: tamanhos maiores tendem a reduzir a variância das atualizações e, com isso, melhorar a convergência — embora com maior custo computacional.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y6LaCzEIgpqr"
      }
    }
  ]
}