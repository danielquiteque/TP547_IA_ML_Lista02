{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UpEaVQcgi5s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qual técnica de regressão linear você usaria se tivesse um conjunto de treinamento\n",
        "com milhares de features (i.e., atributos)? Explique por quais razões você utilizaria esta\n",
        "técnica."
      ],
      "metadata": {
        "id": "9XUCKe6fgjkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diante de um conjunto de dados com milhares de atributos, uma das abordagens mais eficientes para treinar um modelo de regressão linear é utilizar o gradiente descendente estocástico (SGD) ou o gradiente descendente em mini-batches. Isso porque o método tradicional de gradiente descendente em batch completo (full batch) se torna computacionalmente oneroso nesse contexto, já que exige o processamento de todo o conjunto de treinamento a cada atualização de parâmetro, o que pode ser impraticável com grande dimensionalidade.\n",
        "\n",
        "Por outro lado, o gradiente estocástico realiza atualizações mais rápidas ao calcular o gradiente com base em um único exemplo por iteração, o que melhora o desempenho computacional. No entanto, ele pode introduzir mais variação no processo de aprendizagem, resultando em oscilações em torno do ponto ótimo.\n",
        "\n",
        "O método em mini-batches surge como uma alternativa intermediária, combinando a estabilidade do gradiente batch com a agilidade do estocástico. Ele processa pequenos subconjuntos dos dados em cada passo, acelerando o treinamento e melhorando a convergência em comparação ao batch completo. Além disso, esse método é mais flexível, pois pode simular o comportamento tanto do batch quanto do estocástico ao ajustar o tamanho do mini-batch. Por exemplo, um mini-batch igual ao total de dados equivale ao método em batch, enquanto um mini-batch de tamanho um corresponde ao estocástico.\n",
        "\n",
        "Portanto, o uso do gradiente descendente em mini-batches é recomendado nesse cenário, pois oferece um bom equilíbrio entre velocidade de treinamento, consumo de memória e estabilidade da convergência."
      ],
      "metadata": {
        "id": "Y6LaCzEIgpqr"
      }
    }
  ]
}